{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0877d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e04b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "import holoviews as hv\n",
    "import hdf5storage\n",
    "\n",
    "from utig_radar_loading import file_util, stream_util, geo_util, opr_gps_file_generation, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48246b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = True\n",
    "tqdm.pandas()\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6338ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = True\n",
    "cache_dir = \"outputs/file_index.csv\"\n",
    "base_path = \"/kucresis/scratch/data/UTIG\"\n",
    "\n",
    "df_files = file_util.load_file_index_df(base_path, cache_dir, read_cache=use_cache)\n",
    "\n",
    "df_artifacts = file_util.create_artifacts_df(df_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea88e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artifacts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d23f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_by_transect(df_artifacts, streams):\n",
    "    \"\"\"\n",
    "    Group by transects (unique combinations of (prj, set, trn)) and pull out paths\n",
    "    to the desired data streams.\n",
    "\n",
    "    streams is a dictionary mapping names of data categories to a list of acceptable\n",
    "    stream types. For example:\n",
    "    { \"gps\": [\"GPSnc1\", \"GPSnc2\"],\n",
    "      \"radar\": [\"RADnh5\", \"RADnh6\"] }\n",
    "\n",
    "    The resulting dataframe will have two columns per entry in the streams dictionary:\n",
    "    <data category>_stream_type will contain the matched stream type and\n",
    "    <data category>_path will contain the path to the data file.\n",
    "\n",
    "    If multiple matching stream types are available, preference will be given to the\n",
    "    first stream type in the list. If no matching stream types are available, columns\n",
    "    will be filled with NaN.\n",
    "    \"\"\"\n",
    "    \n",
    "    def agg_fn(group):\n",
    "        df = pd.DataFrame(index=[0])\n",
    "        \n",
    "        # Look for requested data streams\n",
    "        for data_category in streams.keys():\n",
    "            df[f\"{data_category}_stream_type\"] = np.nan\n",
    "            df[f\"{data_category}_path\"] = np.nan\n",
    "            \n",
    "            matching_entry = group[(group['stream'].isin(streams[data_category]['stream_types'])) & \\\n",
    "                (group['file_name'].isin(streams[data_category]['file_names']))]\n",
    "            if not matching_entry.empty:\n",
    "                df[f\"{data_category}_stream_type\"] = matching_entry['stream'].values[0]\n",
    "                df[f\"{data_category}_path\"] = matching_entry['full_path'].values[0]\n",
    "\n",
    "        # Add in any other unique keys\n",
    "        for k in group:\n",
    "            if k in ['full_path', 'stream', 'processing_level', 'processing_type']:\n",
    "                continue\n",
    "            \n",
    "            if len(group[k].unique()) == 1:\n",
    "                df[k] = str(group[k].values[0]) # TODO\n",
    "\n",
    "        return df\n",
    "\n",
    "    df = df_artifacts.groupby(['prj', 'set', 'trn']).apply(agg_fn, include_groups=False)\n",
    "    df.index = df.index.droplevel(-1)\n",
    "    return df\n",
    "\n",
    "df_transects = arrange_by_transect(df_artifacts, {\n",
    "    \"gps\": {\"stream_types\": [\"GPSnc1\", \"GPStp2\", \"GPSap3\"], \"file_names\": [\"xds.gz\"]},\n",
    "    \"radar\": {\"stream_types\": [\"RADnh5\", \"RADnh3\", \"RADnh2\", \"RADnh4\", \"RADjh1\"], \"file_names\": [\"bxds\"]}\n",
    "})\n",
    "df_transects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c497e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_timestamp(transect):\n",
    "    # Iterate over stream data until we find one that has a valid context file\n",
    "    \n",
    "    fp = transect['gps_path']\n",
    "    if isinstance(fp, float) and np.isnan(fp):\n",
    "        return None\n",
    "\n",
    "    ct_df = stream_util.load_ct_file(fp, read_csv_kwargs={'nrows': 1})\n",
    "    ct_df = stream_util.parse_CT(ct_df)\n",
    "\n",
    "    return ct_df.iloc[0]['TIMESTAMP']\n",
    "\n",
    "def get_end_timestamp(transect):\n",
    "    fp = transect['gps_path']\n",
    "    \n",
    "    # Read last few bytes and extract last line\n",
    "    with gzip.open(fp, 'rb') as f:\n",
    "        f.seek(-2, os.SEEK_END)\n",
    "        while f.read(1) != b'\\n':\n",
    "            f.seek(-2, os.SEEK_CUR)\n",
    "        last_line = f.readline().decode()\n",
    "    \n",
    "    # Load and parse just the last line\n",
    "    from io import StringIO\n",
    "    ct_columns = ['prj', 'set', 'trn', 'seq', 'clk_y', 'clk_n', 'clk_d', 'clk_h', 'clk_m', 'clk_s', 'clk_f', 'tim']\n",
    "    ct_df = pd.read_csv(StringIO(last_line), sep=r'\\s+', names=ct_columns, index_col=False)\n",
    "    ct_df = stream_util.parse_CT(ct_df)\n",
    "    return ct_df.iloc[0]['TIMESTAMP']\n",
    "\n",
    "def season_from_datetime(d):\n",
    "    if d.month >= 6:\n",
    "        return d.year\n",
    "    else:\n",
    "        return d.year - 1\n",
    "\n",
    "df_all_seasons = df_transects\n",
    "\n",
    "df_all_seasons['start_timestamp'] = df_all_seasons.apply(get_start_timestamp, axis=1)\n",
    "df_all_seasons['season'] = df_all_seasons['start_timestamp'].apply(season_from_datetime)\n",
    "df_all_seasons = df_all_seasons.sort_values('prj')\n",
    "#df_all_seasons.to_csv('tmp.csv')\n",
    "df_all_seasons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ebc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The following seasons were found in the dataset:\")\n",
    "seasons = np.array(list(df_all_seasons['season'].unique()))\n",
    "seasons.sort()\n",
    "print(seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd740f8",
   "metadata": {},
   "source": [
    "### Select a single season to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63816801",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_year = 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c0910",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_season = df_all_seasons[df_all_seasons['season'] == season_year]\n",
    "df_season = df_season.sort_values(by='start_timestamp')\n",
    "\n",
    "df_season_missing_data = df_season[df_season['radar_path'].isnull()]\n",
    "df_season = df_season[df_season['radar_path'].notnull()]\n",
    "\n",
    "print(f\"Missing radar data for {len(df_season_missing_data)} transects out of {len(df_season)+len(df_season_missing_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7925e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_season['gps_stream_type'].unique(), df_season['radar_stream_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4320484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_season.reset_index()['set'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae31d4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_ident = df_season.reset_index()['set'].iloc[0][:3]\n",
    "\n",
    "season_name = f\"{season_year}_Antarctica_Basler{ac_ident}\"\n",
    "season_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aafd49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projects in the season\n",
    "df_season.index.get_level_values(0).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d24fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge segments\n",
    "\n",
    "timestamp_field = 'tim'\n",
    "timestamp_split_threshold = 1000\n",
    "parse_ct = False\n",
    "\n",
    "# timestamp_field = 'TIMESTAMP'\n",
    "# timestamp_split_threshold = pd.Timedelta(milliseconds=10000)\n",
    "# parse_ct = True\n",
    "\n",
    "last_segment_ct = stream_util.load_ct_file(df_season.iloc[0]['radar_path'])\n",
    "if parse_ct:\n",
    "    last_segment_ct = stream_util.parse_CT(last_segment_ct)\n",
    "\n",
    "df_season['segment_path'] = \"\"\n",
    "df_season['segment_date_str'] = \"\"\n",
    "df_season['segment_number'] = -1\n",
    "current_segment_datestring = df_season.iloc[0]['start_timestamp'].strftime(\"%Y%m%d\")\n",
    "current_segment_idx = 1\n",
    "\n",
    "df_season.iloc[0, df_season.columns.get_loc('segment_date_str')] = current_segment_datestring\n",
    "df_season.iloc[0, df_season.columns.get_loc('segment_path')] = f\"{current_segment_datestring}_{current_segment_idx:02d}\"\n",
    "df_season.iloc[0, df_season.columns.get_loc('segment_number')] = current_segment_idx\n",
    "\n",
    "\n",
    "print(f\"Initial segment path is: {df_season.iloc[0]['segment_path']}\")\n",
    "\n",
    "for row_iloc in tqdm(range(1, len(df_season))):\n",
    "    try:\n",
    "        curr_segment_ct = stream_util.load_ct_file(df_season.iloc[row_iloc]['radar_path'])\n",
    "        if parse_ct:\n",
    "            curr_segment_ct = stream_util.parse_CT(curr_segment_ct)\n",
    "\n",
    "        delta_from_last = curr_segment_ct[timestamp_field].iloc[0] - last_segment_ct[timestamp_field].iloc[-1]\n",
    "\n",
    "        if np.abs(delta_from_last) > timestamp_split_threshold:\n",
    "            new_datestring = df_season.iloc[row_iloc]['start_timestamp'].strftime(\"%Y%m%d\")\n",
    "            if new_datestring == current_segment_datestring:\n",
    "                current_segment_idx += 1\n",
    "            else:\n",
    "                current_frame_idx = 1\n",
    "                current_segment_idx = 1\n",
    "                current_segment_datestring = new_datestring\n",
    "\n",
    "            print(f\"Segment path changed to {current_segment_datestring}_{current_segment_idx:02d}. Delta in '{timestamp_field}' was {delta_from_last}\")\n",
    "\n",
    "        df_season.iloc[row_iloc, df_season.columns.get_loc('segment_date_str')] = current_segment_datestring\n",
    "        df_season.iloc[row_iloc, df_season.columns.get_loc('segment_path')] = f\"{current_segment_datestring}_{current_segment_idx:02d}\"\n",
    "        df_season.iloc[row_iloc, df_season.columns.get_loc('segment_number')] = current_segment_idx\n",
    "\n",
    "        last_segment_ct = curr_segment_ct\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load index {row_iloc}\")\n",
    "        print(df_season.iloc[row_iloc]['radar_path'])\n",
    "        print(e)\n",
    "\n",
    "df_season.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_season_missing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025bc3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gps_data(transects_df):\n",
    "    segment_dfs = []\n",
    "\n",
    "    for _, row in tqdm(transects_df.iterrows(), total=len(transects_df)):\n",
    "\n",
    "        f = row['gps_path']\n",
    "        \n",
    "        df = stream_util.load_gzipped_stream_file(f, debug=False, parse=True, parse_kwargs={'use_ct': True})\n",
    "\n",
    "        line_length_km = stream_util.calculate_track_distance_km(df)\n",
    "\n",
    "        _, _, line_length_m_shapely = geo_util.project_split_and_simplify(df['LON'].values, df['LAT'].values, calc_length=True, simplify_tolerance=100)\n",
    "\n",
    "        necessary_keys = ['prj', 'set', 'trn', 'clk_y', 'LAT', 'LON', 'TIMESTAMP']\n",
    "        for k in necessary_keys:\n",
    "            if k not in df:\n",
    "                df[k] = np.nan\n",
    "\n",
    "        df_sub = df[['prj', 'set', 'trn', 'clk_y', 'LAT', 'LON', 'TIMESTAMP']]\n",
    "\n",
    "        if 'segment_path' in row:\n",
    "            df_sub['segment_path'] = row['segment_path']\n",
    "\n",
    "        segment_dfs.append(df_sub)\n",
    "    return segment_dfs\n",
    "\n",
    "segment_dfs = load_gps_data(df_season)\n",
    "missing_data_dfs = load_gps_data(df_season_missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6654f35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_segments = len(df_season['segment_path'].unique())\n",
    "max_segments_per_day = df_season['segment_number'].max()\n",
    "\n",
    "print(f\"Created {n_segments} segments. Maximum segment number on a single day is {max_segments_per_day}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a7faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "\n",
    "# Add missing data\n",
    "if len(missing_data_dfs) > 0:\n",
    "    _, p = geo_util.create_path(missing_data_dfs)\n",
    "    paths.append(p.opts(color='red', line_width=3).relabel('Missing Radar Data'))\n",
    "else:\n",
    "    print(\"No missing data to display.\")\n",
    "\n",
    "# Add segments with data\n",
    "for segment_path in df_season['segment_path'].unique():\n",
    "    dfs_list_tmp = [df for df in segment_dfs if df['segment_path'].iloc[0] == segment_path]\n",
    "    _, p = geo_util.create_path(dfs_list_tmp)\n",
    "    p = p.relabel(f\"Segment {segment_path}\")\n",
    "    paths.append(p)\n",
    "\n",
    "p = stream_util.create_antarctica_basemap() * hv.Overlay(paths)\n",
    "p = p.opts(aspect='equal', frame_width=800, frame_height=800, tools=['hover'])\n",
    "p = p.opts(title=season_name, legend_position='right')\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e09f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.save(p, f\"outputs/maps/{season_name}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfca731",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_season[df_season['segment_path'] == '20190114_01']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51498922",
   "metadata": {},
   "source": [
    "## Create GPS files for each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62403ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_segment_gps_file(x):\n",
    "    x = x.reset_index()\n",
    "    print(f\"{x['segment_date_str'].iloc[0]}_{x['segment_number'].iloc[0]}\")\n",
    "    gps_paths = list(x['gps_path'].unique())\n",
    "    output_path = f\"outputs/gps/{season_name}/gps_{x['segment_date_str'].iloc[0]}_{x['segment_number'].iloc[0]}.mat\"\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    # Only generate if the file does not exist\n",
    "    if not output_path.exists():\n",
    "        opr_gps_file_generation.generate_gps_file(gps_paths, output_path, format='hdf5')\n",
    "    else:\n",
    "        print(f\"File {output_path} already exists. Skipping generation. If you want to regenerate, manually delete the file.\")\n",
    "\n",
    "    return output_path.resolve()\n",
    "\n",
    "gps_paths = df_season.groupby(['segment_date_str', 'segment_number'])[['segment_date_str', 'segment_number', 'gps_path']].apply(make_segment_gps_file, include_groups=False)\n",
    "gps_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c67df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def radar_paths_ordered(x):\n",
    "    l = x.sort_values('start_timestamp')['radar_path'].tolist()\n",
    "    l = [str(Path(*Path(p).parts[-5:-1])) for p in l]\n",
    "    return \"{'\" + \"', '\".join(l) + \"'}\"\n",
    "\n",
    "radar_paths = df_season.groupby(['segment_date_str', 'segment_number'])[['radar_path', 'start_timestamp']].apply(radar_paths_ordered)\n",
    "radar_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fda4b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_prj_set_str(x):\n",
    "    return list(x['prj'].unique())\n",
    "\n",
    "def transect_names(x):\n",
    "    return list(x.sort_values('start_timestamp')['trn'])\n",
    "\n",
    "mission_names = df_season.reset_index().groupby(['segment_date_str', 'segment_number'])[['prj', 'set']].apply(first_prj_set_str)\n",
    "transect_names = df_season.reset_index().groupby(['segment_date_str', 'segment_number'])[['start_timestamp', 'trn']].apply(transect_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2122e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = preprocessing.load_defaults('src/utig_radar_loading/defaults/2018_Antarctica_BaslerJKB.yaml')\n",
    "\n",
    "def make_parameter_sheet(default_values, segments, overrides={}):\n",
    "    df = pd.DataFrame(default_values, index=segments)\n",
    "    for key, value in overrides.items():\n",
    "       df[key] = value\n",
    "    return df\n",
    "\n",
    "make_parameter_sheet(defaults['cmd'], radar_paths.index, overrides={\n",
    "    'mission_names': mission_names,\n",
    "    'notes': transect_names\n",
    "}).to_csv('outputs/params/cmd.csv')\n",
    "\n",
    "make_parameter_sheet(defaults['records'], radar_paths.index, overrides={\n",
    "    'file.board_folder_name': radar_paths,\n",
    "    'gps.fn': gps_paths\n",
    "}).to_csv('outputs/params/records.csv')\n",
    "\n",
    "make_parameter_sheet(defaults['qlook'], radar_paths.index).to_csv('outputs/params/qlook.csv')\n",
    "make_parameter_sheet(defaults['radar'], radar_paths.index).to_csv('outputs/params/radar.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5865d5",
   "metadata": {},
   "source": [
    "### Generate temporary header files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec1aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_season.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46caad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_header_information(f):\n",
    "    ct_data = stream_util.load_ct_file(f)\n",
    "    ct_data = stream_util.parse_CT(ct_data)\n",
    "    headers = {\n",
    "        # comp_time is ct_data['TIMESTAMP'] converted to a floating point unix timestamp\n",
    "        'comp_time': ((ct_data['TIMESTAMP'] - pd.Timestamp(\"1970-01-01\")) / pd.Timedelta('1s')).values.astype(np.float64),\n",
    "        # radar_time is ct_data['tim']\n",
    "        'radar_time': ct_data['tim'].values,\n",
    "    }\n",
    "    #print(f\"comp_time[0]={headers['comp_time'][0]}, comp_time[-1]={headers['comp_time'][-1]}\")\n",
    "    #print(f\"radar_time[0]={headers['radar_time'][0]}, radar_time[-1]={headers['radar_time'][-1]}\")\n",
    "\n",
    "    headers['offset'] = np.arange(len(ct_data)) # TODO: Unclear to me what 'offset' is supposed to be\n",
    "    return headers\n",
    "\n",
    "def get_header_file_location(f, base_dir=f\"/kucresis/scratch/tteisberg_sta/scripts/opr_user_tmp/headers/rds/{season_name}/\"):\n",
    "    p = Path(f)\n",
    "    fn_name = p.stem\n",
    "    board_folder_name = Path(*p.parts[-5:-1])\n",
    "    board_folder_name_cur = base_dir / board_folder_name\n",
    "    return str(board_folder_name_cur / (fn_name + '.mat'))\n",
    "\n",
    "\n",
    "headers = df_season['radar_path'].progress_apply(get_header_information)\n",
    "header_file_locations = df_season['radar_path'].apply(get_header_file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc8db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for header, fn in zip(headers.values, header_file_locations.values):\n",
    "    fn = Path(fn)\n",
    "    fn.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Writing header to {fn}\")\n",
    "    hdf5storage.savemat(str(fn), header, format='7.3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a7780e",
   "metadata": {},
   "source": [
    "## Break segments into frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a65379",
   "metadata": {},
   "outputs": [],
   "source": [
    "break_distance = 50 # km\n",
    "\n",
    "frame_outputs = {}\n",
    "all_entries = []\n",
    "\n",
    "segment_paths = df_season['segment_path'].unique()\n",
    "for seg in segment_paths:\n",
    "    print(f\"Processing segment: {seg}\")\n",
    "    seg_df = df_season[df_season['segment_path'] == seg].sort_values('start_timestamp')\n",
    "    # Note: Should have already been sorted, but just in case\n",
    "\n",
    "    frame_idx = 1 # Frame index we're currently assigning\n",
    "    accumulated_km = 0 # Sum of line-km currently assigned to frame_idx\n",
    "    transect_iloc = 0 # Index of the current transect being processed\n",
    "\n",
    "    frame_outputs[seg] = {frame_idx: []}\n",
    "    last_x, last_y = None, None\n",
    "\n",
    "    for transect_iloc in range(len(seg_df)):\n",
    "        print(f\" -> Allocating transect {transect_iloc} {seg_df.index[transect_iloc]}\")\n",
    "\n",
    "        # Load the geometry of this transect\n",
    "        df = stream_util.load_gzipped_stream_file(\n",
    "            seg_df.iloc[transect_iloc]['gps_path'],\n",
    "            debug=False, parse=True, parse_kwargs={'use_ct': True}\n",
    "            )\n",
    "\n",
    "        x_proj, y_proj, line_length_m = geo_util.project_split_and_simplify(\n",
    "            df['LON'].values, df['LAT'].values, calc_length=True, simplify_tolerance=None)\n",
    "        \n",
    "        x_proj = x_proj[:-1]\n",
    "        y_proj = y_proj[:-1]\n",
    "\n",
    "        # Calculate the along-track distance, accounting for possible distance from the\n",
    "        # end of the last transect\n",
    "        deltas = np.sqrt(np.diff(x_proj)**2 + np.diff(y_proj)**2) / 1000  # Convert to km\n",
    "        if last_x:\n",
    "            deltas = np.insert(deltas, 0, np.sqrt((x_proj[0] - last_x)**2 + (y_proj[0] - last_y)**2) / 1000)\n",
    "        else:\n",
    "            deltas = np.insert(deltas, 0, 0)\n",
    "        dist = np.cumsum(deltas)\n",
    "        #print(f\"Transect total length is {dist[-1]} km\")\n",
    "        # print(x_proj)\n",
    "        # print(y_proj)\n",
    "        # print(dist)\n",
    "        # raise Exception(\"test\")\n",
    "\n",
    "        # Allocate parts of this transect to frames\n",
    "        transect_start_tim = df['tim'].iloc[0]\n",
    "        transect_start_idx = 0\n",
    "        while transect_start_tim < df['tim'].iloc[-1]:\n",
    "            # Find the 'tim' index that fits into the current segment\n",
    "            remaining_distance = break_distance - accumulated_km\n",
    "\n",
    "            dists_from_idx = np.maximum(0, dist - dist[transect_start_idx])\n",
    "            #print(f\"With transect_start_idx={transect_start_idx}, remaining distance in this transect is {dists_from_idx[-1]} km\")\n",
    "\n",
    "            break_idx = np.argmin(np.abs(dists_from_idx - remaining_distance))\n",
    "            break_tim = df['tim'].iloc[break_idx]\n",
    "\n",
    "            entry = seg_df.iloc[transect_iloc:transect_iloc+1].copy()\n",
    "            entry['gps_idx_start'] = transect_start_idx\n",
    "            entry['gps_idx_stop'] = break_idx\n",
    "            entry['tim_start'] = transect_start_tim\n",
    "            entry['tim_stop'] = break_tim\n",
    "            entry['frame_number'] = frame_idx\n",
    "\n",
    "            all_entries.append(entry)\n",
    "\n",
    "            # Add an entry to this frame and update distance\n",
    "            frame_outputs[seg][frame_idx].append(entry)\n",
    "            accumulated_km += dist[break_idx] - dist[transect_start_idx]\n",
    "            print(f\"   -> Assigned indices {transect_start_idx} to {break_idx} (distance {dist[break_idx] - dist[transect_start_idx]} km) to frame {frame_idx}, now at {accumulated_km} km\")\n",
    "\n",
    "            # Move transect start index\n",
    "            transect_start_idx = break_idx\n",
    "            transect_start_tim = break_tim\n",
    "\n",
    "            # Check if the frame is full\n",
    "            if accumulated_km >= 0.98*break_distance:\n",
    "                print(f\"    Frame {frame_idx} is full with {accumulated_km} km\")\n",
    "                frame_idx += 1\n",
    "                accumulated_km = 0\n",
    "                frame_outputs[seg][frame_idx] = []\n",
    "            \n",
    "\n",
    "        last_x, last_y = x_proj[-1], y_proj[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8efc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_plan_df = pd.concat(all_entries).reset_index().set_index(['segment_date_str', 'segment_number', 'frame_number'])\n",
    "frames_plan_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be43edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_segment_gps_file(x):\n",
    "    x = x.reset_index()\n",
    "    print(f\"{x['segment_date_str'].iloc[0]}_{x['segment_number'].iloc[0]}\")\n",
    "    gps_paths = list(x['gps_path'].unique())\n",
    "    output_path = f\"outputs/gps/{season_name}/gps_{x['segment_date_str'].iloc[0]}_{x['segment_number'].iloc[0]}.mat\"\n",
    "\n",
    "    # Only generate if the file does not exist\n",
    "    if not Path(output_path).exists():\n",
    "        opr_gps_file_generation.generate_gps_file(gps_paths, output_path, format='hdf5')\n",
    "    else:\n",
    "        print(f\"File {output_path} already exists. Skipping generation. If you want to regenerate, manually delete the file.\")\n",
    "        return None\n",
    "\n",
    "    return output_path\n",
    "\n",
    "frames_plan_df.groupby(['segment_date_str', 'segment_number']).apply(make_segment_gps_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fa0666",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_plan_df_tmp = frames_plan_df[:3]\n",
    "frames_plan_df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c6c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e4374a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract headers from bxds files\n",
    "bxds_files = list(frames_plan_df_tmp['radar_path'].unique())\n",
    "headers = preprocessing.extract_headers(bxds_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f6177",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = preprocessing.create_segments_from_frames(\n",
    "      frames_plan_df_tmp,\n",
    "      bxds_files,\n",
    "      headers\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500251f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract headers (if needed for records_create)\n",
    "#   headers = preprocessing.extract_headers(bxds_files)\n",
    "\n",
    "#   # Create segments from existing frames\n",
    "#   segments = preprocessing.create_segments_from_frames(\n",
    "#       frames_plan_df,\n",
    "#       bxds_files,\n",
    "#       headers\n",
    "#   )\n",
    "\n",
    "#   # Generate parameters\n",
    "#   params = preprocessing.generate_all_parameters(\n",
    "#       segments,\n",
    "#       season_name='2022_Antarctica_BaslerMKB',\n",
    "#       radar_name='rds',\n",
    "#       defaults_file='path/to/2022_Antarctica_BaslerMKB.yaml',\n",
    "#       base_dir='/data/path',\n",
    "#       board_folder_name='F01'\n",
    "#   )\n",
    "\n",
    "#   # Write spreadsheets\n",
    "#   preprocessing.write_parameter_spreadsheet(\n",
    "#       params,\n",
    "#       'output/2022_Antarctica_BaslerMKB_param'\n",
    "#   )\n",
    "\n",
    "#   # Save headers for MATLAB records_create\n",
    "#   preprocessing.save_temporary_headers(\n",
    "#       headers,\n",
    "#       bxds_files,\n",
    "#       Path('/opr_tmp'),\n",
    "#       '2022_Antarctica_BaslerMKB',\n",
    "#       'F01'\n",
    "#   )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-load-utig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
