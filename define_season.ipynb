{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0877d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e04b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "import holoviews as hv\n",
    "import hdf5storage\n",
    "\n",
    "import unfoc\n",
    "\n",
    "from utig_radar_loading import file_util, stream_util, geo_util, segment_splits, opr_gps_file_generation, opr_header_generation, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48246b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = True\n",
    "tqdm.pandas()\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6338ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = True\n",
    "cache_dir = \"outputs/file_index.csv\"\n",
    "base_path = \"/kucresis/scratch/data/UTIG\"\n",
    "\n",
    "df_files = file_util.load_file_index_df(base_path, cache_dir, read_cache=use_cache)\n",
    "df_artifacts = file_util.create_artifacts_df(df_files) # df_artifacts is a dataframe with one row per stream file\n",
    "\n",
    "df_artifacts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d23f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by transect, selecting the stream types that are needed\n",
    "\n",
    "usable_artifact_types = {\n",
    "    \"gps\": {\"stream_types\": [\"GPSnc1\", \"GPStp2\", \"GPSap3\"], \"file_names\": [\"xds.gz\"]},\n",
    "    \"radar\": {\"stream_types\": [\"RADnh5\", \"RADnh3\", \"RADnh2\", \"RADnh4\", \"RADjh1\"], \"file_names\": [\"bxds\"]},\n",
    "    \"imu\": {\"stream_types\": [\"AVNnp1\"], \"file_names\": [\"bxds\"]},\n",
    "}\n",
    "\n",
    "df_transects = file_util.arrange_by_transect(df_artifacts, usable_artifact_types)\n",
    "df_transects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c497e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_seasons = file_util.assign_seasons(df_transects)\n",
    "df_all_seasons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ebc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The following seasons were found in the dataset:\")\n",
    "seasons = np.array(df_all_seasons['season'].unique())\n",
    "seasons.sort()\n",
    "print(seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd740f8",
   "metadata": {},
   "source": [
    "### Select a single season to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63816801",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_year = 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c0910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df_season (filtered to selected season) and check for missing data\n",
    "\n",
    "df_season = df_all_seasons[df_all_seasons['season'] == season_year]\n",
    "df_season = df_season.sort_values(by='start_timestamp')\n",
    "\n",
    "df_season_missing_data = df_season[df_season['radar_path'].isnull()]\n",
    "df_season = df_season[df_season['radar_path'].notnull()]\n",
    "\n",
    "if len(df_season_missing_data) > 0:\n",
    "    print(f\"[WARNING] Missing radar data for {len(df_season_missing_data)} transects out of {len(df_season)+len(df_season_missing_data)}\")\n",
    "\n",
    "df_season_missing_imu = df_season[df_season['imu_path'].isnull()]\n",
    "if len(df_season_missing_imu) > 0:\n",
    "    print(f\"[WARNING] Missing IMU data for {len(df_season_missing_imu)} transects\")\n",
    "\n",
    "# Display information about this season\n",
    "\n",
    "# - Types of stream files:\n",
    "print(f\"GPS stream types: {df_season['gps_stream_type'].unique()}\")\n",
    "print(f\"Radar stream types: {df_season['radar_stream_type'].unique()}\")\n",
    "print(f\"IMU stream types: {df_season['imu_stream_type'].unique()}\")\n",
    "\n",
    "# - Sets\n",
    "print(f\"Sets: {df_season.reset_index()['set'].unique()}\")\n",
    "\n",
    "# - Projects\n",
    "print(f\"Projects: {df_season.reset_index()['prj'].unique()}\")\n",
    "\n",
    "# - Aircraft\n",
    "ac_ident = df_season.reset_index()['set'].iloc[0][:3]\n",
    "print(f\"Aircraft identifier: {ac_ident}\")\n",
    "\n",
    "# - Season name\n",
    "season_name = f\"{season_year}_Antarctica_Basler{ac_ident}\"\n",
    "print(f\"Season name: {season_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d24fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each transect to a segment\n",
    "\n",
    "df_season = segment_splits.assign_segments(df_season, timestamp_field='tim', parse_ct=False, timestamp_split_threshold=2000) # Split based on the 'tim' counter (in ms)\n",
    "#df_season = segment_splits.assign_segments(df_season, timestamp_field='TIMESTAMP', parse_ct=True, timestamp_split_threshold=pd.Timedelta(milliseconds=2000)) # OR, split based on the 'TIMESTAMP' field\n",
    "\n",
    "n_segments = len(df_season['segment_path'].unique())\n",
    "max_segments_per_day = df_season['segment_number'].max()\n",
    "\n",
    "print(f\"Created {n_segments} segments. Maximum segment number on a single day is {max_segments_per_day}.\")\n",
    "df_season.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a7faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create map of segments\n",
    "\n",
    "segment_dfs = geo_util.load_gps_data(df_season)\n",
    "missing_data_dfs = geo_util.load_gps_data(df_season_missing_data)\n",
    "\n",
    "paths = []\n",
    "\n",
    "# Add missing data\n",
    "if len(missing_data_dfs) > 0:\n",
    "    _, p = geo_util.create_path(missing_data_dfs)\n",
    "    paths.append(p.opts(color='red', line_width=3).relabel('Missing Radar Data'))\n",
    "else:\n",
    "    print(\"No missing data to display.\")\n",
    "\n",
    "# Add segments with data\n",
    "for segment_path in df_season['segment_path'].unique():\n",
    "    dfs_list_tmp = [df for df in segment_dfs if df['segment_path'].iloc[0] == segment_path]\n",
    "    _, p = geo_util.create_path(dfs_list_tmp)\n",
    "    p = p.relabel(f\"Segment {segment_path}\")\n",
    "    paths.append(p)\n",
    "\n",
    "p = stream_util.create_antarctica_basemap() * hv.Overlay(paths)\n",
    "p = p.opts(aspect='equal', frame_width=800, frame_height=800, tools=['hover'])\n",
    "p = p.opts(title=season_name, legend_position='right')\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e09f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.save(p, f\"outputs/maps/{season_name}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51498922",
   "metadata": {},
   "source": [
    "### Create GPS support files for each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20cd051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gps_df = stream_util.load_xds_stream_file(df_season.iloc[5]['gps_path'])\n",
    "# imu_df = stream_util.parse_binary_AVNnp1(df_season.iloc[5]['imu_path'])\n",
    "\n",
    "# print(gps_df.columns)\n",
    "# print(imu_df.columns)\n",
    "\n",
    "# gps_time_keys = ['TIMESTAMP', 'GPS_TIME']\n",
    "# imu_time_keys = ['GPS_TIME', 'unix_time']\n",
    "\n",
    "# for key in gps_time_keys:\n",
    "#     if key in gps_df.columns:\n",
    "#         raw_range = gps_df[key].max() - gps_df[key].min()\n",
    "#         print(f\"GPS {key}: min={gps_df[key].min()}, max={gps_df[key].max()}\")\n",
    "#         print(f\"  type: {gps_df[key].dtype}, raw range: {raw_range}\")\n",
    "#         if np.issubdtype(gps_df[key].dtype, np.number):\n",
    "#             print(f\"   {raw_range} seconds = {raw_range/60} minutes\")\n",
    "\n",
    "#             date_since_gps_epoch = pd.to_datetime('1980-01-06 00:00:00') + pd.to_timedelta(gps_df[key], unit='s')\n",
    "#             print(f\"   which is {date_since_gps_epoch.min()} to {date_since_gps_epoch.max()} UTC (assuming GPS epoch)\")\n",
    "#             date_since_unix_epoch = pd.to_datetime(gps_df[key], unit='s', origin='unix')\n",
    "#             print(f\"   which is {date_since_unix_epoch.min()} to {date_since_unix_epoch.max()} UTC (assuming Unix epoch)\")\n",
    "#             date_since_ni_epoch = pd.to_datetime('1904-01-01 00:00:00') + pd.to_timedelta(gps_df[key], unit='s')\n",
    "#             print(f\"   which is {date_since_ni_epoch.min()} to {date_since_ni_epoch.max()} UTC (assuming NI epoch)\")\n",
    "\n",
    "# for key in imu_time_keys:\n",
    "#     if key in imu_df.columns:\n",
    "#         raw_range = imu_df[key].max() - imu_df[key].min()\n",
    "#         print(f\"IMU {key}: min={imu_df[key].min()}, max={imu_df[key].max()}\")\n",
    "#         print(f\"  type: {imu_df[key].dtype}, raw range: {raw_range}\")\n",
    "#         if np.issubdtype(imu_df[key].dtype, np.number):\n",
    "#             print(f\"   {raw_range} seconds = {raw_range/60} minutes\")\n",
    "\n",
    "#             date_since_gps_epoch = pd.to_datetime('1980-01-06 00:00:00') + pd.to_timedelta(imu_df[key], unit='s')\n",
    "#             print(f\"   which is {date_since_gps_epoch.min()} to {date_since_gps_epoch.max()} UTC (assuming GPS epoch)\")\n",
    "#             date_since_unix_epoch = pd.to_datetime(imu_df[key], unit='s', origin='unix')\n",
    "#             print(f\"   which is {date_since_unix_epoch.min()} to {date_since_unix_epoch.max()} UTC (assuming Unix epoch)\")\n",
    "#             date_since_ni_epoch = pd.to_datetime('1904-01-01 00:00:00') + pd.to_timedelta(imu_df[key], unit='s')\n",
    "#             print(f\"   which is {date_since_ni_epoch.min()} to {date_since_ni_epoch.max()} UTC (assuming NI epoch)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62403ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_existing_gps_files = False\n",
    "\n",
    "gps_paths = df_season.groupby(['segment_date_str', 'segment_number'])[['segment_date_str', 'segment_number', 'gps_path', 'imu_path']].apply(\n",
    "    opr_gps_file_generation.make_segment_gps_file,\n",
    "    include_groups=False,\n",
    "    output_base_dir=f\"outputs/gps/{season_name}\",\n",
    "    overwrite=overwrite_existing_gps_files)\n",
    "\n",
    "gps_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78080d0a",
   "metadata": {},
   "source": [
    "### Create parameter spreadsheet starting templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c67df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def radar_paths_ordered(x):\n",
    "    l = x.sort_values('start_timestamp')['radar_path'].tolist()\n",
    "    l = [str(Path(*Path(p).parts[-5:-1])) for p in l]\n",
    "    return \"{'\" + \"', '\".join(l) + \"'}\"\n",
    "\n",
    "radar_paths = df_season.groupby(['segment_date_str', 'segment_number'])[['radar_path', 'start_timestamp']].apply(radar_paths_ordered)\n",
    "radar_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fda4b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_prj_set_str(x):\n",
    "    return list(x['prj'].unique())\n",
    "\n",
    "def transect_names(x):\n",
    "    return list(x.sort_values('start_timestamp')['trn'])\n",
    "\n",
    "mission_names = df_season.reset_index().groupby(['segment_date_str', 'segment_number'])[['prj', 'set']].apply(first_prj_set_str)\n",
    "transect_names = df_season.reset_index().groupby(['segment_date_str', 'segment_number'])[['start_timestamp', 'trn']].apply(transect_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2122e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = preprocessing.load_defaults(f'src/utig_radar_loading/defaults/{season_name}.yaml')\n",
    "\n",
    "base_params_dir = Path(f'outputs/params/{season_name}')\n",
    "Path(base_params_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def make_parameter_sheet(default_values, segments, overrides={}):\n",
    "    df = pd.DataFrame(default_values, index=segments)\n",
    "    for key, value in overrides.items():\n",
    "       df[key] = value\n",
    "    return df\n",
    "\n",
    "make_parameter_sheet(defaults['cmd'], radar_paths.index, overrides={\n",
    "    'mission_names': mission_names,\n",
    "    'notes': transect_names\n",
    "}).to_csv(base_params_dir / 'cmd.csv')\n",
    "\n",
    "make_parameter_sheet(defaults['records'], radar_paths.index, overrides={\n",
    "    'file.board_folder_name': radar_paths,\n",
    "    'gps.fn': gps_paths\n",
    "}).to_csv(base_params_dir / 'records.csv')\n",
    "\n",
    "make_parameter_sheet(defaults['qlook'], radar_paths.index).to_csv(base_params_dir / 'qlook.csv')\n",
    "make_parameter_sheet(defaults['radar'], radar_paths.index).to_csv(base_params_dir / 'radar.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5865d5",
   "metadata": {},
   "source": [
    "### Generate temporary header files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712e80a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_season_subset = df_season #.loc[[('PEL', 'JKB2u', 'Y20b'), ('ICP10', 'JKB2u', 'F01T02a'), ('PEL', 'JKB2u', 'X48a')]]\n",
    "df_season_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d84c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, as_completed, progress\n",
    "from dask import delayed\n",
    "\n",
    "print(\"Setting up Dask LocalCluster for parallel processing...\")\n",
    "client = Client(n_workers=10)\n",
    "print(f\"Dashboard link: {client.dashboard_link}\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46caad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_base_dir = f\"/kucresis/scratch/tteisberg_sta/scripts/opr_user_tmp/headers/rds/{season_name}/\"\n",
    "\n",
    "# Get file locations (this is fast, no need to parallelize)\n",
    "header_file_locations = df_season_subset['radar_path'].apply(opr_header_generation.get_header_file_location, base_dir=header_base_dir)\n",
    "\n",
    "# Function to get header and save it\n",
    "def get_and_save_header(path, fn):\n",
    "    \"\"\"Get header information and save it to file\"\"\"\n",
    "    header = opr_header_generation.get_header_information(path)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    fn_path = Path(fn)\n",
    "    fn_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save to file\n",
    "    hdf5storage.savemat(str(fn_path), header, format='7.3')\n",
    "    print(f\"Saved header to {fn}\")\n",
    "    \n",
    "    return header\n",
    "\n",
    "# Parallelized version using Dask\n",
    "# Create delayed tasks for each file\n",
    "delayed_tasks = []\n",
    "files_to_process = []\n",
    "\n",
    "for path, fn in zip(df_season_subset['radar_path'], header_file_locations):\n",
    "    if Path(fn).exists():\n",
    "        print(f\"Header file already exists for {path}, skipping.\")\n",
    "    else:\n",
    "        delayed_tasks.append(delayed(get_and_save_header)(path, fn))\n",
    "        files_to_process.append((path, fn))\n",
    "\n",
    "if len(delayed_tasks) > 0:\n",
    "    # Compute in parallel with progress bar\n",
    "    print(f\"Processing and saving {len(delayed_tasks)} header files in parallel...\")\n",
    "    \n",
    "    # Submit all tasks\n",
    "    futures = client.compute(delayed_tasks)\n",
    "    \n",
    "    # Track progress\n",
    "    progress(futures)\n",
    "    \n",
    "    # Gather results\n",
    "    headers_list = client.gather(futures)\n",
    "    \n",
    "    print(f\"Successfully generated and saved {len(headers_list)} header files.\")\n",
    "else:\n",
    "    print(\"No header files need to be generated, all files already exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc8db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(header_file_locations_to_generate) > 0:\n",
    "#     for header, fn in zip(headers.values, header_file_locations_to_generate.values()):\n",
    "#         fn = Path(fn)\n",
    "#         fn.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#         header_tmp = header.copy()\n",
    "#         header_tmp['offset'] = header_tmp['offset'].astype(np.int64)  # Convert offsets to int64 for saving -- TODO: should have been this type originally\n",
    "\n",
    "#         print(f\"Writing header to {fn}\")\n",
    "#         hdf5storage.savemat(str(fn), header_tmp, format='7.3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-load-utig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
