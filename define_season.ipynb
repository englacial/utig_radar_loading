{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0877d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e04b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "import holoviews as hv\n",
    "import hdf5storage\n",
    "\n",
    "import unfoc\n",
    "\n",
    "from utig_radar_loading import file_util, stream_util, geo_util, segment_splits, opr_gps_file_generation, opr_header_generation, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48246b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = True\n",
    "tqdm.pandas()\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6338ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = True\n",
    "cache_dir = \"outputs/file_index.csv\"\n",
    "base_path = \"/kucresis/scratch/data/UTIG\"\n",
    "\n",
    "df_files = file_util.load_file_index_df(base_path, cache_dir, read_cache=use_cache)\n",
    "df_artifacts = file_util.create_artifacts_df(df_files) # df_artifacts is a dataframe with one row per stream file\n",
    "\n",
    "df_artifacts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d23f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by transect, selecting the stream types that are needed\n",
    "\n",
    "usable_artifact_types = {\n",
    "    \"gps\": {\"stream_types\": [\"GPSnc1\", \"GPStp2\", \"GPSap3\"], \"file_names\": [\"xds.gz\"]},\n",
    "    \"radar\": {\"stream_types\": [\"RADnh5\", \"RADnh3\", \"RADnh2\", \"RADnh4\", \"RADjh1\"], \"file_names\": [\"bxds\"]},\n",
    "    \"imu\": {\"stream_types\": [\"AVNnp1\"], \"file_names\": [\"bxds\"]},\n",
    "}\n",
    "\n",
    "df_transects = file_util.arrange_by_transect(df_artifacts, usable_artifact_types)\n",
    "df_transects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c497e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_seasons = file_util.assign_seasons(df_transects)\n",
    "df_all_seasons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ebc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The following seasons were found in the dataset:\")\n",
    "seasons = np.array(df_all_seasons['season'].unique())\n",
    "seasons.sort()\n",
    "print(seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd740f8",
   "metadata": {},
   "source": [
    "### Select a single season to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63816801",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_year = 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c0910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df_season (filtered to selected season) and check for missing data\n",
    "\n",
    "df_season = df_all_seasons[df_all_seasons['season'] == season_year]\n",
    "df_season = df_season.sort_values(by='start_timestamp')\n",
    "\n",
    "df_season_missing_data = df_season[df_season['radar_path'].isnull()]\n",
    "df_season = df_season[df_season['radar_path'].notnull()]\n",
    "\n",
    "if len(df_season_missing_data) > 0:\n",
    "    print(f\"[WARNING] Missing radar data for {len(df_season_missing_data)} transects out of {len(df_season)+len(df_season_missing_data)}\")\n",
    "\n",
    "df_season_missing_imu = df_season[df_season['imu_path'].isnull()]\n",
    "if len(df_season_missing_imu) > 0:\n",
    "    print(f\"[WARNING] Missing IMU data for {len(df_season_missing_imu)} transects\")\n",
    "\n",
    "# Display information about this season\n",
    "\n",
    "# - Types of stream files:\n",
    "print(f\"GPS stream types: {df_season['gps_stream_type'].unique()}\")\n",
    "print(f\"Radar stream types: {df_season['radar_stream_type'].unique()}\")\n",
    "print(f\"IMU stream types: {df_season['imu_stream_type'].unique()}\")\n",
    "\n",
    "# - Sets\n",
    "print(f\"Sets: {df_season.reset_index()['set'].unique()}\")\n",
    "\n",
    "# - Projects\n",
    "print(f\"Projects: {df_season.reset_index()['prj'].unique()}\")\n",
    "\n",
    "# - Aircraft\n",
    "ac_ident = df_season.reset_index()['set'].iloc[0][:3]\n",
    "print(f\"Aircraft identifier: {ac_ident}\")\n",
    "\n",
    "# - Season name\n",
    "season_name = f\"{season_year}_Antarctica_Basler{ac_ident}\"\n",
    "print(f\"Season name: {season_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d24fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each transect to a segment\n",
    "\n",
    "df_season = segment_splits.assign_segments(df_season, timestamp_field='tim', parse_ct=False, timestamp_split_threshold=2000) # Split based on the 'tim' counter (in ms)\n",
    "#df_season = segment_splits.assign_segments(df_season, timestamp_field='TIMESTAMP', parse_ct=True, timestamp_split_threshold=pd.Timedelta(milliseconds=2000)) # OR, split based on the 'TIMESTAMP' field\n",
    "\n",
    "n_segments = len(df_season['segment_path'].unique())\n",
    "max_segments_per_day = df_season['segment_number'].max()\n",
    "\n",
    "print(f\"Created {n_segments} segments. Maximum segment number on a single day is {max_segments_per_day}.\")\n",
    "df_season.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a7faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create map of segments\n",
    "\n",
    "segment_dfs = geo_util.load_gps_data(df_season)\n",
    "missing_data_dfs = geo_util.load_gps_data(df_season_missing_data)\n",
    "\n",
    "paths = []\n",
    "\n",
    "# Add missing data\n",
    "if len(missing_data_dfs) > 0:\n",
    "    _, p = geo_util.create_path(missing_data_dfs)\n",
    "    paths.append(p.opts(color='red', line_width=3).relabel('Missing Radar Data'))\n",
    "else:\n",
    "    print(\"No missing data to display.\")\n",
    "\n",
    "# Add segments with data\n",
    "for segment_path in df_season['segment_path'].unique():\n",
    "    dfs_list_tmp = [df for df in segment_dfs if df['segment_path'].iloc[0] == segment_path]\n",
    "    _, p = geo_util.create_path(dfs_list_tmp)\n",
    "    p = p.relabel(f\"Segment {segment_path}\")\n",
    "    paths.append(p)\n",
    "\n",
    "p = stream_util.create_antarctica_basemap() * hv.Overlay(paths)\n",
    "p = p.opts(aspect='equal', frame_width=800, frame_height=800, tools=['hover'])\n",
    "p = p.opts(title=season_name, legend_position='right')\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e09f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.save(p, f\"outputs/maps/{season_name}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51498922",
   "metadata": {},
   "source": [
    "### Create GPS support files for each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62403ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_existing_gps_files = False\n",
    "\n",
    "gps_paths = df_season.groupby(['segment_date_str', 'segment_number'])[['segment_date_str', 'segment_number', 'gps_path']].apply(\n",
    "    opr_gps_file_generation.make_segment_gps_file,\n",
    "    include_groups=False,\n",
    "    output_base_dir=f\"outputs/gps/{season_name}\",\n",
    "    overwrite=overwrite_existing_gps_files)\n",
    "\n",
    "gps_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78080d0a",
   "metadata": {},
   "source": [
    "### Create parameter spreadsheet starting templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c67df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def radar_paths_ordered(x):\n",
    "    l = x.sort_values('start_timestamp')['radar_path'].tolist()\n",
    "    l = [str(Path(*Path(p).parts[-5:-1])) for p in l]\n",
    "    return \"{'\" + \"', '\".join(l) + \"'}\"\n",
    "\n",
    "radar_paths = df_season.groupby(['segment_date_str', 'segment_number'])[['radar_path', 'start_timestamp']].apply(radar_paths_ordered)\n",
    "radar_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fda4b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_prj_set_str(x):\n",
    "    return list(x['prj'].unique())\n",
    "\n",
    "def transect_names(x):\n",
    "    return list(x.sort_values('start_timestamp')['trn'])\n",
    "\n",
    "mission_names = df_season.reset_index().groupby(['segment_date_str', 'segment_number'])[['prj', 'set']].apply(first_prj_set_str)\n",
    "transect_names = df_season.reset_index().groupby(['segment_date_str', 'segment_number'])[['start_timestamp', 'trn']].apply(transect_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2122e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = preprocessing.load_defaults(f'src/utig_radar_loading/defaults/{season_name}.yaml')\n",
    "\n",
    "base_params_dir = Path(f'outputs/params/{season_name}')\n",
    "Path(base_params_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def make_parameter_sheet(default_values, segments, overrides={}):\n",
    "    df = pd.DataFrame(default_values, index=segments)\n",
    "    for key, value in overrides.items():\n",
    "       df[key] = value\n",
    "    return df\n",
    "\n",
    "make_parameter_sheet(defaults['cmd'], radar_paths.index, overrides={\n",
    "    'mission_names': mission_names,\n",
    "    'notes': transect_names\n",
    "}).to_csv(base_params_dir / 'cmd.csv')\n",
    "\n",
    "make_parameter_sheet(defaults['records'], radar_paths.index, overrides={\n",
    "    'file.board_folder_name': radar_paths,\n",
    "    'gps.fn': gps_paths\n",
    "}).to_csv(base_params_dir / 'records.csv')\n",
    "\n",
    "make_parameter_sheet(defaults['qlook'], radar_paths.index).to_csv(base_params_dir / 'qlook.csv')\n",
    "make_parameter_sheet(defaults['radar'], radar_paths.index).to_csv(base_params_dir / 'radar.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5865d5",
   "metadata": {},
   "source": [
    "### Generate temporary header files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712e80a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_season_subset = df_season #.loc[[('PEL', 'JKB2u', 'Y20b'), ('ICP10', 'JKB2u', 'F01T02a'), ('PEL', 'JKB2u', 'X48a')]]\n",
    "df_season_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d84c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, as_completed, progress\n",
    "from dask import delayed\n",
    "\n",
    "print(\"Setting up Dask LocalCluster for parallel processing...\")\n",
    "client = Client(n_workers=10)\n",
    "print(f\"Dashboard link: {client.dashboard_link}\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46caad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_base_dir = f\"/kucresis/scratch/tteisberg_sta/scripts/opr_user_tmp/headers/rds/{season_name}/\"\n",
    "\n",
    "# Get file locations (this is fast, no need to parallelize)\n",
    "header_file_locations = df_season_subset['radar_path'].apply(opr_header_generation.get_header_file_location, base_dir=header_base_dir)\n",
    "\n",
    "# Parallelized version using Dask\n",
    "\n",
    "# Create delayed tasks for each file\n",
    "header_file_locations_to_generate = []\n",
    "delayed_tasks = []\n",
    "for path, fn in zip(df_season_subset['radar_path'], header_file_locations):\n",
    "    if Path(fn).exists():\n",
    "        print(f\"Header file already exists for {path}, skipping.\")\n",
    "    else:\n",
    "        delayed_tasks.append(delayed(opr_header_generation.get_header_information)(path))\n",
    "        header_file_locations_to_generate.append(fn)\n",
    "\n",
    "if len(header_file_locations_to_generate) > 0:\n",
    "    # Compute in parallel with progress bar\n",
    "    print(f\"Processing {len(delayed_tasks)} files in parallel...\")\n",
    "    headers_list = []\n",
    "\n",
    "    # Submit all tasks\n",
    "    futures = client.compute(delayed_tasks)\n",
    "\n",
    "    # Track progress\n",
    "    progress(futures)\n",
    "\n",
    "    # Gather results\n",
    "    headers_list = client.gather(futures)\n",
    "\n",
    "    # Convert to Series matching original index\n",
    "    headers = pd.Series(headers_list, index=df_season_subset.index)\n",
    "else:\n",
    "    print(\"No header files need to be generated, all files already exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc8db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Seems like this hangs or behaves weirdly when overwriting an existing file...\n",
    "\n",
    "if len(header_file_locations_to_generate) > 0:\n",
    "    for header, fn in zip(headers.values, header_file_locations_to_generate.values()):\n",
    "        fn = Path(fn)\n",
    "        fn.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        header_tmp = header.copy()\n",
    "        header_tmp['offset'] = header_tmp['offset'].astype(np.int64)  # Convert offsets to int64 for saving -- TODO: should have been this type originally\n",
    "\n",
    "        print(f\"Writing header to {fn}\")\n",
    "        hdf5storage.savemat(str(fn), header_tmp, format='7.3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7a4d44",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "Just got updated headers writing with the new Nb x Nx x Nc offset format. Next step is to regenreate records files in matlab and see if the records look correct. Then need to update data load pathways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "header['offset'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed226b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for header, fn in zip(headers.values, header_file_locations.values):\n",
    "    fn = Path(fn)\n",
    "    fn.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[DRYRUN] Writing header to {fn}\")\n",
    "\n",
    "    header_tmp = header.copy()\n",
    "    header_tmp['offset'] = header_tmp['offset'].astype(np.int64)  # Convert offsets to int64 for saving\n",
    "    \n",
    "    for k, v in header_tmp.items():\n",
    "        print(f\"  {k}: {v.shape} {v.dtype} itemsize={v.itemsize} total size={v.nbytes / (1024*1024):.2f} MB \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f727e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = df_season_subset.iloc[0]['radar_path']\n",
    "ct_data = stream_util.load_ct_file(input_filename)\n",
    "ct_data = stream_util.parse_CT(ct_data)\n",
    "fpos, header_len, header = zip(*unfoc.index_RADnhx_bxds(input_filename, full_header=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01376b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpos, header_len, header = zip(*unfoc.index_RADnhx_bxds(input_filename, full_header=True))\n",
    "\n",
    "rseq = np.array([h.rseq for h in header])\n",
    "choff = np.array([h.choff for h in header])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'rseq': rseq,\n",
    "    'choff': choff,\n",
    "    'start_fpos': fpos,\n",
    "    'header_len': header_len,\n",
    "    'ch0_offset': pd.NA,\n",
    "    'ch1_offset': pd.NA,\n",
    "    'ch2_offset': pd.NA,\n",
    "    'ch3_offset': pd.NA\n",
    "})\n",
    "\n",
    "df = df.join(ct_data[['tim', 'TIMESTAMP']])\n",
    "\n",
    "choff0 = np.where(df['choff'] == 0)[0]\n",
    "choff2 = np.where(df['choff'] == 2)[0]\n",
    "df.iloc[choff0, df.columns.get_loc('ch0_offset')] = df.iloc[choff0]['start_fpos'] + df.iloc[choff0]['header_len']\n",
    "df.iloc[choff0, df.columns.get_loc('ch1_offset')] = df.iloc[choff0]['start_fpos'] + df.iloc[choff0]['header_len'] + 6400\n",
    "df.iloc[choff2, df.columns.get_loc('ch2_offset')] = df.iloc[choff2]['start_fpos'] + df.iloc[choff2]['header_len']\n",
    "df.iloc[choff2, df.columns.get_loc('ch3_offset')] = df.iloc[choff2]['start_fpos'] + df.iloc[choff2]['header_len'] + 6400\n",
    "\n",
    "df = df[['tim', 'TIMESTAMP', 'rseq', 'ch0_offset', 'ch1_offset', 'ch2_offset', 'ch3_offset']]\n",
    "# Group by rseq and get first non-NAN value for each channel\n",
    "df = df.groupby('rseq').first()\n",
    "# Set nan values to -2^31\n",
    "df = df.fillna(-2**31)\n",
    "\n",
    "df\n",
    "\n",
    "#offsets_array = df_offsets.to_numpy()\n",
    "#offsets_array = np.expand_dims(offsets_array, axis=0) # Add a board axis\n",
    "\n",
    "#offsets_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742a9b1b",
   "metadata": {},
   "source": [
    "# STOP HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a7780e",
   "metadata": {},
   "source": [
    "## Break segments into frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a65379",
   "metadata": {},
   "outputs": [],
   "source": [
    "break_distance = 50 # km\n",
    "\n",
    "frame_outputs = {}\n",
    "all_entries = []\n",
    "\n",
    "segment_paths = df_season['segment_path'].unique()\n",
    "for seg in segment_paths:\n",
    "    print(f\"Processing segment: {seg}\")\n",
    "    seg_df = df_season[df_season['segment_path'] == seg].sort_values('start_timestamp')\n",
    "    # Note: Should have already been sorted, but just in case\n",
    "\n",
    "    frame_idx = 1 # Frame index we're currently assigning\n",
    "    accumulated_km = 0 # Sum of line-km currently assigned to frame_idx\n",
    "    transect_iloc = 0 # Index of the current transect being processed\n",
    "\n",
    "    frame_outputs[seg] = {frame_idx: []}\n",
    "    last_x, last_y = None, None\n",
    "\n",
    "    for transect_iloc in range(len(seg_df)):\n",
    "        print(f\" -> Allocating transect {transect_iloc} {seg_df.index[transect_iloc]}\")\n",
    "\n",
    "        # Load the geometry of this transect\n",
    "        df = stream_util.load_gzipped_stream_file(\n",
    "            seg_df.iloc[transect_iloc]['gps_path'],\n",
    "            debug=False, parse=True, parse_kwargs={'use_ct': True}\n",
    "            )\n",
    "\n",
    "        x_proj, y_proj, line_length_m = geo_util.project_split_and_simplify(\n",
    "            df['LON'].values, df['LAT'].values, calc_length=True, simplify_tolerance=None)\n",
    "        \n",
    "        x_proj = x_proj[:-1]\n",
    "        y_proj = y_proj[:-1]\n",
    "\n",
    "        # Calculate the along-track distance, accounting for possible distance from the\n",
    "        # end of the last transect\n",
    "        deltas = np.sqrt(np.diff(x_proj)**2 + np.diff(y_proj)**2) / 1000  # Convert to km\n",
    "        if last_x:\n",
    "            deltas = np.insert(deltas, 0, np.sqrt((x_proj[0] - last_x)**2 + (y_proj[0] - last_y)**2) / 1000)\n",
    "        else:\n",
    "            deltas = np.insert(deltas, 0, 0)\n",
    "        dist = np.cumsum(deltas)\n",
    "        #print(f\"Transect total length is {dist[-1]} km\")\n",
    "        # print(x_proj)\n",
    "        # print(y_proj)\n",
    "        # print(dist)\n",
    "        # raise Exception(\"test\")\n",
    "\n",
    "        # Allocate parts of this transect to frames\n",
    "        transect_start_tim = df['tim'].iloc[0]\n",
    "        transect_start_idx = 0\n",
    "        while transect_start_tim < df['tim'].iloc[-1]:\n",
    "            # Find the 'tim' index that fits into the current segment\n",
    "            remaining_distance = break_distance - accumulated_km\n",
    "\n",
    "            dists_from_idx = np.maximum(0, dist - dist[transect_start_idx])\n",
    "            #print(f\"With transect_start_idx={transect_start_idx}, remaining distance in this transect is {dists_from_idx[-1]} km\")\n",
    "\n",
    "            break_idx = np.argmin(np.abs(dists_from_idx - remaining_distance))\n",
    "            break_tim = df['tim'].iloc[break_idx]\n",
    "\n",
    "            entry = seg_df.iloc[transect_iloc:transect_iloc+1].copy()\n",
    "            entry['gps_idx_start'] = transect_start_idx\n",
    "            entry['gps_idx_stop'] = break_idx\n",
    "            entry['tim_start'] = transect_start_tim\n",
    "            entry['tim_stop'] = break_tim\n",
    "            entry['frame_number'] = frame_idx\n",
    "\n",
    "            all_entries.append(entry)\n",
    "\n",
    "            # Add an entry to this frame and update distance\n",
    "            frame_outputs[seg][frame_idx].append(entry)\n",
    "            accumulated_km += dist[break_idx] - dist[transect_start_idx]\n",
    "            print(f\"   -> Assigned indices {transect_start_idx} to {break_idx} (distance {dist[break_idx] - dist[transect_start_idx]} km) to frame {frame_idx}, now at {accumulated_km} km\")\n",
    "\n",
    "            # Move transect start index\n",
    "            transect_start_idx = break_idx\n",
    "            transect_start_tim = break_tim\n",
    "\n",
    "            # Check if the frame is full\n",
    "            if accumulated_km >= 0.98*break_distance:\n",
    "                print(f\"    Frame {frame_idx} is full with {accumulated_km} km\")\n",
    "                frame_idx += 1\n",
    "                accumulated_km = 0\n",
    "                frame_outputs[seg][frame_idx] = []\n",
    "            \n",
    "\n",
    "        last_x, last_y = x_proj[-1], y_proj[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8efc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_plan_df = pd.concat(all_entries).reset_index().set_index(['segment_date_str', 'segment_number', 'frame_number'])\n",
    "frames_plan_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be43edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_segment_gps_file(x):\n",
    "    x = x.reset_index()\n",
    "    print(f\"{x['segment_date_str'].iloc[0]}_{x['segment_number'].iloc[0]}\")\n",
    "    gps_paths = list(x['gps_path'].unique())\n",
    "    output_path = f\"outputs/gps/{season_name}/gps_{x['segment_date_str'].iloc[0]}_{x['segment_number'].iloc[0]}.mat\"\n",
    "\n",
    "    # Only generate if the file does not exist\n",
    "    if not Path(output_path).exists():\n",
    "        opr_gps_file_generation.generate_gps_file(gps_paths, output_path, format='hdf5')\n",
    "    else:\n",
    "        print(f\"File {output_path} already exists. Skipping generation. If you want to regenerate, manually delete the file.\")\n",
    "        return None\n",
    "\n",
    "    return output_path\n",
    "\n",
    "frames_plan_df.groupby(['segment_date_str', 'segment_number']).apply(make_segment_gps_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fa0666",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_plan_df_tmp = frames_plan_df[:3]\n",
    "frames_plan_df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c6c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e4374a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract headers from bxds files\n",
    "bxds_files = list(frames_plan_df_tmp['radar_path'].unique())\n",
    "headers = preprocessing.extract_headers(bxds_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f6177",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = preprocessing.create_segments_from_frames(\n",
    "      frames_plan_df_tmp,\n",
    "      bxds_files,\n",
    "      headers\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500251f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract headers (if needed for records_create)\n",
    "#   headers = preprocessing.extract_headers(bxds_files)\n",
    "\n",
    "#   # Create segments from existing frames\n",
    "#   segments = preprocessing.create_segments_from_frames(\n",
    "#       frames_plan_df,\n",
    "#       bxds_files,\n",
    "#       headers\n",
    "#   )\n",
    "\n",
    "#   # Generate parameters\n",
    "#   params = preprocessing.generate_all_parameters(\n",
    "#       segments,\n",
    "#       season_name='2022_Antarctica_BaslerMKB',\n",
    "#       radar_name='rds',\n",
    "#       defaults_file='path/to/2022_Antarctica_BaslerMKB.yaml',\n",
    "#       base_dir='/data/path',\n",
    "#       board_folder_name='F01'\n",
    "#   )\n",
    "\n",
    "#   # Write spreadsheets\n",
    "#   preprocessing.write_parameter_spreadsheet(\n",
    "#       params,\n",
    "#       'output/2022_Antarctica_BaslerMKB_param'\n",
    "#   )\n",
    "\n",
    "#   # Save headers for MATLAB records_create\n",
    "#   preprocessing.save_temporary_headers(\n",
    "#       headers,\n",
    "#       bxds_files,\n",
    "#       Path('/opr_tmp'),\n",
    "#       '2022_Antarctica_BaslerMKB',\n",
    "#       'F01'\n",
    "#   )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-load-utig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
