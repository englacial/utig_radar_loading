{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0877d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e04b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "import holoviews as hv\n",
    "import hdf5storage\n",
    "\n",
    "import unfoc\n",
    "\n",
    "from utig_radar_loading import file_util, stream_util, geo_util, segment_splits, opr_gps_file_generation, opr_header_generation, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48246b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = True\n",
    "tqdm.pandas()\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1d4b6f",
   "metadata": {},
   "source": [
    "# Step 0: Indexing all files\n",
    "\n",
    "This stage indexes all of the UTIG data files that can be found and creates a DataFrame of \"artifacts\" corresponding to each individual data file.\n",
    "\n",
    "This artifacts DataFrame is then processed to group it by transect (P/S/T triples in UTIG terminology). Finally, some data is read from the context files to assign seasons to each artifact. Once seasons are defined, a list of possible season years is printed and the user can select a year to process for the following stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6338ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = True\n",
    "cache_dir = \"outputs/file_index.csv\"\n",
    "base_path = \"/kucresis/scratch/data/UTIG\"\n",
    "\n",
    "df_files = file_util.load_file_index_df(base_path, cache_dir, read_cache=use_cache)\n",
    "df_artifacts = file_util.create_artifacts_df(df_files) # df_artifacts is a dataframe with one row per stream file\n",
    "\n",
    "df_artifacts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f39f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Debug\n",
    "\n",
    "# Find all transects matching a specific prj and trn\n",
    "prj, trn = 'ASB', 'R04Ea'\n",
    "df_tmp = df_artifacts[(df_artifacts['prj'] == prj) & (df_artifacts['trn'] == trn)]\n",
    "df_tmp\n",
    "#df_tmp = df_tmp[df_tmp['stream'] == 'GPStp2']\n",
    "#df_tmp.iloc[0]['full_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d23f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by transect, selecting the stream types that are needed\n",
    "\n",
    "usable_artifact_types = {\n",
    "    \"gps\": {\"stream_types\": [\"GPSnc1\", \"GPStp2\", \"GPSap3\", \"GPSkc1\"], \"file_names\": [\"xds.gz\"]},\n",
    "    \"radar\": {\"stream_types\": [\"RADnh5\", \"RADnh3\", \"RADnh2\", \"RADnh4\", \"RADjh1\"], \"file_names\": [\"bxds\", \"bxds1\"]}, # We assume that if a bxds1 file exists, it implies a bxds2 file\n",
    "    \"imu\": {\"stream_types\": [\"AVNnp1\"], \"file_names\": [\"bxds\"]},\n",
    "}\n",
    "\n",
    "df_transects = file_util.arrange_by_transect(df_artifacts, usable_artifact_types)\n",
    "\n",
    "# If needed, find streams from other sets to include\n",
    "add_from_other_sets = True\n",
    "if add_from_other_sets:\n",
    "    usable_artifact_types_other_sets = {\n",
    "        \"gps_with_position\": {\"stream_types\": [\"GPSnc1\", \"GPStp2\", \"GPSap3\"], \"file_names\": [\"xds.gz\"]},\n",
    "    }\n",
    "    df_transects_other_sets = file_util.arrange_by_transect(df_artifacts, usable_artifact_types_other_sets, ignore_set=True)\n",
    "    df_transects_other_sets = df_transects_other_sets.drop(columns = [c for c in df_transects_other_sets if c in set(df_transects.columns)])\n",
    "    df_transects_other_sets = df_transects_other_sets.rename(columns={'set': 'other_set_source'})\n",
    "\n",
    "    # Merge them into df_transects\n",
    "    df_transects = df_transects.join(df_transects_other_sets, on=['prj', 'trn'])\n",
    "\n",
    "\n",
    "df_transects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f93b5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_transects[df_transects.index == ('ASB', 'R04Ea')]['gps_path'].values[0] # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c497e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_seasons = file_util.assign_seasons(df_transects)\n",
    "df_all_seasons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ebc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The following seasons were found in the dataset:\")\n",
    "seasons = np.array(df_all_seasons['season'].unique())\n",
    "seasons.sort()\n",
    "print(seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd740f8",
   "metadata": {},
   "source": [
    "# Step 1: Select a single season to extract\n",
    "\n",
    "From here on, the rest of this workflow operates on a single season at a time. The season is identified initially by a year, corresponding to the year at the start of the season.\n",
    "\n",
    "If available, `season_gps_postprocessed_dir` should be set to the path of the post-processed GPS and IMU data. If it is not available, it can be set to None:\n",
    "\n",
    "```\n",
    "season_gps_postprocessed_dir = \"/resfs/GROUPS/CRESIS/dataproducts/metadata/2015_Antarctica_BaslerJKB/gps\" # Preferred if available\n",
    "season_gps_postprocessed_dir = None # Data can be processed from field GPS and IMU data only if that's all that's available\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63816801",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_year = 2008 # Want: 2008 -- waiting on GPS data\n",
    "\n",
    "season_gps_postprocessed_dir = \"/resfs/GROUPS/CRESIS/dataproducts/metadata/2008_Antarctica_BaslerJKB/gps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_season = df_all_seasons[df_all_seasons['season'] == season_year]\n",
    "df_season = df_season.sort_index()\n",
    "\n",
    "# Combine rows with matching prj and trn, but different set - take the row with radar data if available. If both rows have radar data, print a warning.\n",
    "df_reset = df_season.reset_index()\n",
    "\n",
    "# Group by prj and trn\n",
    "grouped = df_reset.groupby(['prj', 'trn'])\n",
    "\n",
    "rows_to_keep = []\n",
    "for (prj, trn), group in grouped:\n",
    "    if len(group) == 1:\n",
    "        # No duplicates, keep as is\n",
    "        rows_to_keep.append(group.iloc[0])\n",
    "    else:\n",
    "        # Multiple rows with same prj/trn but different set\n",
    "        rows_with_radar = group[group['radar_path'].notna()]\n",
    "        \n",
    "        if len(rows_with_radar) == 0:\n",
    "            # No radar data in any row, keep the first one\n",
    "            rows_to_keep.append(group.iloc[0])\n",
    "        elif len(rows_with_radar) == 1:\n",
    "            # Exactly one row with radar data, keep that one\n",
    "            rows_to_keep.append(rows_with_radar.iloc[0])\n",
    "        else:\n",
    "            # Multiple rows with radar data - print warning and keep first one with radar\n",
    "            print(f\"[WARNING] Multiple rows with radar data for prj={prj}, trn={trn}:\")\n",
    "            print(f\"  Sets: {rows_with_radar['set'].tolist()}\")\n",
    "            rows_to_keep.append(rows_with_radar.iloc[0])\n",
    "\n",
    "# Reconstruct the dataframe\n",
    "df_season = pd.DataFrame(rows_to_keep)\n",
    "df_season = df_season.set_index(['prj', 'set', 'trn'])\n",
    "df_season = df_season.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c0910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df_season (filtered to selected season) and check for missing data\n",
    "\n",
    "#df_season = df_all_seasons[df_all_seasons['season'] == season_year]\n",
    "#df_season = df_season.sort_index()\n",
    "\n",
    "# Add post-processed GPS paths\n",
    "if season_gps_postprocessed_dir is not None:\n",
    "    df_season = file_util.add_postprocessed_gps_paths(df_season, season_gps_postprocessed_dir, ignore_set=True)\n",
    "\n",
    "    # Update start timestamps based on post-processed GPS data if available\n",
    "    df_season['start_timestamp'] = df_season.apply(file_util.get_start_timestamp, axis=1)\n",
    "else:\n",
    "    df_season['postprocessed_gps_path'] = None\n",
    "\n",
    "# Check for timing-only GPS streams\n",
    "only_timing_mask = (df_season['gps_stream_type'] == 'GPSkc1') & (df_season['postprocessed_gps_path'].isnull())\n",
    "df_season_timing_only = df_season[only_timing_mask]\n",
    "if len(df_season_timing_only) > 0:\n",
    "    print(f\"[WARNING] There are {len(df_season_timing_only)} transects with only timing-only GPS data (GPSkc1) and no post-processed GPS data.\")\n",
    "    df_season = df_season[~only_timing_mask]\n",
    "\n",
    "# Handle missing radar data\n",
    "df_season_missing_data = df_season[df_season['radar_path'].isnull()]\n",
    "df_season = df_season[df_season['radar_path'].notnull()]\n",
    "\n",
    "if len(df_season_missing_data) > 0:\n",
    "    print(f\"[WARNING] Missing radar data for {len(df_season_missing_data)} transects out of {len(df_season)+len(df_season_missing_data)}\")\n",
    "    if np.any(df_season_missing_data['gps_stream_type'] == 'GPSkc1'): # Timing only GPS type\n",
    "        gpskc1_only_count = np.sum(df_season_missing_data['gps_stream_type'] == 'GPSkc1')\n",
    "        print(f\"  - Of these, {gpskc1_only_count} transects only have GPSkc1 timing-only GPS data\")\n",
    "    if np.any(df_season_missing_data['postprocessed_gps_path'].notnull()):\n",
    "        postprocessed_gps_count = np.sum(df_season_missing_data['postprocessed_gps_path'].notnull())\n",
    "        print(f\"  - Of these, {postprocessed_gps_count} transects have post-processed GPS data available\")\n",
    "        print(df_season_missing_data[df_season_missing_data['postprocessed_gps_path'].notnull()].index)\n",
    "\n",
    "# Handle missing post-processed GPS data\n",
    "df_season_missing_postprocessed_gps = df_season[df_season['postprocessed_gps_path'].isnull()]\n",
    "if season_gps_postprocessed_dir is not None and len(df_season_missing_postprocessed_gps) > 0:\n",
    "    print(f\"[WARNING] Missing post-processed GPS data for {len(df_season_missing_postprocessed_gps)} transects\")\n",
    "\n",
    "    df_season = df_season[df_season['postprocessed_gps_path'].notnull()]\n",
    "\n",
    "# Display information about this season\n",
    "\n",
    "# - Types of stream files:\n",
    "print(f\"GPS stream types: {df_season['gps_stream_type'].unique()}\")\n",
    "print(f\"Radar stream types: {df_season['radar_stream_type'].unique()}\")\n",
    "print(f\"IMU stream types: {df_season['imu_stream_type'].unique()}\")\n",
    "\n",
    "# - Sets\n",
    "print(f\"Sets: {df_season.reset_index()['set'].unique()}\")\n",
    "\n",
    "# - Projects\n",
    "print(f\"Projects: {df_season.reset_index()['prj'].unique()}\")\n",
    "\n",
    "# - Aircraft\n",
    "ac_ident = df_season.reset_index()['set'].iloc[0][:3]\n",
    "print(f\"Aircraft identifier: {ac_ident}\")\n",
    "\n",
    "# - Season name\n",
    "season_name = f\"{season_year}_Antarctica_Basler{ac_ident}\"\n",
    "\n",
    "if False: # TODO: For testing only -- limit to first 5 segments\n",
    "    season_name += \"TEST2\" # TODO: Remove after testing\n",
    "    #df_season = df_season.head(5)\n",
    "    #print(\"[WARNING] Limiting to first 5 segments for testing purposes\")\n",
    "print(f\"Season name: {season_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dce9058y7m",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for postprocessed GPS files without corresponding radar data in df_season\n",
    "if season_gps_postprocessed_dir is not None:\n",
    "    # Get all postprocessed GPS files in the directory\n",
    "    gps_files = glob.glob(f\"{season_gps_postprocessed_dir}/*PUTG1B_*_position.txt\")\n",
    "    \n",
    "    # Parse filenames and check against df_season\n",
    "    unmatched_gps_files = []\n",
    "    \n",
    "    for f in gps_files:\n",
    "        parts = Path(f).stem.split('_')\n",
    "        if len(parts) < 6:\n",
    "            continue\n",
    "        prj = parts[-4]\n",
    "        set_ = parts[-3]\n",
    "        trn = parts[-2]\n",
    "        \n",
    "        # Check if this PRJ/TRN exists in df_season with radar data\n",
    "        # Note: df_season uses ignore_set=True for GPS matching, so we check by prj and trn\n",
    "        matching_rows = df_season.reset_index()\n",
    "        matching_rows = matching_rows[(matching_rows['prj'] == prj) & (matching_rows['trn'] == trn)]\n",
    "        \n",
    "        # Get first timestamp from the GPS file\n",
    "        first_timestamp = None\n",
    "        try:\n",
    "            gps_df = opr_gps_file_generation.load_and_parse_postprocessed_gps_file(f)\n",
    "            if len(gps_df) > 0:\n",
    "                first_timestamp = pd.Timestamp.fromtimestamp(gps_df.iloc[0]['GPS_TIME'])\n",
    "        except Exception as e:\n",
    "            first_timestamp = f\"Error: {str(e)[:30]}\"\n",
    "        \n",
    "        if len(matching_rows) == 0:\n",
    "            # No match at all in df_season\n",
    "            unmatched_gps_files.append({\n",
    "                'file': Path(f).name,\n",
    "                'prj': prj,\n",
    "                'set': set_,\n",
    "                'trn': trn,\n",
    "                'first_timestamp': first_timestamp,\n",
    "                'reason': 'Not in df_season (not in this season or filtered out)'\n",
    "            })\n",
    "        elif matching_rows['radar_path'].isna().all():\n",
    "            # Match exists but no radar data\n",
    "            unmatched_gps_files.append({\n",
    "                'file': Path(f).name,\n",
    "                'prj': prj,\n",
    "                'set': set_,\n",
    "                'trn': trn,\n",
    "                'first_timestamp': first_timestamp,\n",
    "                'reason': 'No radar data'\n",
    "            })\n",
    "    \n",
    "    if len(unmatched_gps_files) > 0:\n",
    "        print(f\"\\nFound {len(unmatched_gps_files)} postprocessed GPS files without corresponding radar data in df_season:\")\n",
    "        print(\"-\" * 130)\n",
    "        print(f\"  {'Filename':<60} | {'PRJ':>5} {'SET':>6} {'TRN':<10} | {'First Timestamp':<19} | {'Reason'}\")\n",
    "        print(\"-\" * 130)\n",
    "        for item in unmatched_gps_files:\n",
    "            ts_str = str(item['first_timestamp'])[:19] if item['first_timestamp'] else 'N/A'\n",
    "            print(f\"  {item['file']:<60} | {item['prj']:>5} {item['set']:>6} {item['trn']:<10} | {ts_str:<19} | {item['reason']}\")\n",
    "    else:\n",
    "        print(\"\\nAll postprocessed GPS files in the directory have corresponding radar data in df_season.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d24fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each transect to a segment\n",
    "\n",
    "df_season = segment_splits.assign_segments(df_season, timestamp_field='tim', parse_ct=False, timestamp_split_threshold=2000) # Split based on the 'tim' counter (in ms)\n",
    "#df_season = segment_splits.assign_segments(df_season, timestamp_field='TIMESTAMP', parse_ct=True, timestamp_split_threshold=pd.Timedelta(milliseconds=2000)) # OR, split based on the 'TIMESTAMP' field\n",
    "\n",
    "n_segments = len(df_season['segment_path'].unique())\n",
    "max_segments_per_day = df_season['segment_number'].max()\n",
    "\n",
    "print(f\"Created {n_segments} segments. Maximum segment number on a single day is {max_segments_per_day}.\")\n",
    "df_season.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7686f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create map of segments\n",
    "\n",
    "missing_data_dfs = geo_util.load_gps_data(df_season_missing_data.dropna(subset=['gps_path']))\n",
    "\n",
    "only_field_segment_dfs = geo_util.load_gps_data(df_season_missing_postprocessed_gps, source_type='field')\n",
    "\n",
    "if season_gps_postprocessed_dir is not None:\n",
    "    segment_dfs = geo_util.load_gps_data(df_season, source_type='postprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a7faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "\n",
    "# Add missing data\n",
    "if len(missing_data_dfs) > 0:\n",
    "    _, p = geo_util.create_path(missing_data_dfs)\n",
    "    paths.append(p.opts(color='red', line_width=2).relabel('Missing Radar Data'))\n",
    "else:\n",
    "    print(\"No missing data to display.\")\n",
    "\n",
    "if len(only_field_segment_dfs) > 0:\n",
    "    _, p = geo_util.create_path(only_field_segment_dfs)\n",
    "    paths.append(p.opts(color='purple', line_width=2).relabel('Field GPS Data Only'))\n",
    "\n",
    "# Add segments with data\n",
    "if season_gps_postprocessed_dir is not None:\n",
    "    _, p = geo_util.create_path(segment_dfs)\n",
    "    paths.append(p.opts(color='blue', line_width=1).relabel('Radar + Post-processed GPS Data'))\n",
    "\n",
    "# Individually labelled segments (doesn't work well for large number of segments)\n",
    "# for segment_path in df_season['segment_path'].unique():\n",
    "#     dfs_list_tmp = [df for df in segment_dfs if df['segment_path'].iloc[0] == segment_path]\n",
    "#     _, p = geo_util.create_path(dfs_list_tmp)\n",
    "#     p = p.relabel(f\"Segment {segment_path}\")\n",
    "#     paths.append(p)\n",
    "\n",
    "p = stream_util.create_antarctica_basemap() * hv.Overlay(paths)\n",
    "p = p.opts(aspect='equal', frame_width=800, frame_height=800, tools=['hover'])\n",
    "p = p.opts(title=season_name, legend_position='right')\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e09f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.save(p, f\"outputs/maps/{season_name}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51498922",
   "metadata": {},
   "source": [
    "### Create GPS support files for each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb608ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e70a0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO DEBUG FORCE USING FIELD GPS\n",
    "\n",
    "# df_tmp = df_season.rename(columns={\n",
    "#         \"gps_path\": \"gps_path_timing_only\",\n",
    "#         \"gps_stream_type\": \"gps_stream_type_timing_only\"\n",
    "#     }).rename(columns={\n",
    "#         \"gps_with_position_path\": \"gps_path\",\n",
    "#         \"gps_with_position_stream_type\": \"gps_stream_type\"\n",
    "#     })\n",
    "\n",
    "# gps_paths = df_tmp.groupby(['segment_date_str', 'segment_number'])[['segment_date_str', 'segment_number', 'gps_path']].apply(\n",
    "#     opr_gps_file_generation.make_segment_gps_file,\n",
    "#     include_groups=False,\n",
    "#     output_base_dir=f\"outputs/gps/{season_name}\",\n",
    "#     overwrite=True)\n",
    "\n",
    "# gps_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62403ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_existing_gps_files = False\n",
    "\n",
    "gps_paths = df_season.groupby(['segment_date_str', 'segment_number'])[['segment_date_str', 'segment_number', 'gps_path', 'postprocessed_gps_path']].apply(\n",
    "    opr_gps_file_generation.make_segment_gps_file,\n",
    "    include_groups=False,\n",
    "    output_base_dir=f\"outputs/gps/{season_name}\",\n",
    "    overwrite=overwrite_existing_gps_files)\n",
    "\n",
    "gps_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78080d0a",
   "metadata": {},
   "source": [
    "### Create parameter spreadsheet starting templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c67df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def radar_paths_ordered(x):\n",
    "    l = x.sort_values('start_timestamp')['radar_path'].tolist()\n",
    "    l = [str(Path(*Path(p).parts[-5:-1])) for p in l]\n",
    "    return \"{'\" + \"', '\".join(l) + \"'}\"\n",
    "\n",
    "radar_paths = df_season.groupby(['segment_date_str', 'segment_number'])[['radar_path', 'start_timestamp']].apply(radar_paths_ordered)\n",
    "radar_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fda4b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_prj_set_str(x):\n",
    "    return list(x['prj'].unique())\n",
    "\n",
    "def transect_names(x):\n",
    "    return list(x.sort_values('start_timestamp')['trn'])\n",
    "\n",
    "mission_names = df_season.reset_index().groupby(['segment_date_str', 'segment_number'])[['prj', 'set']].apply(first_prj_set_str)\n",
    "transect_names = df_season.reset_index().groupby(['segment_date_str', 'segment_number'])[['start_timestamp', 'trn']].apply(transect_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2122e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = preprocessing.load_defaults(f'seasons_config/{season_name}.yaml')\n",
    "\n",
    "base_params_dir = Path(f'outputs/params/{season_name}')\n",
    "Path(base_params_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def make_parameter_sheet(default_values, segments, overrides={}):\n",
    "    df = pd.DataFrame(default_values, index=segments)\n",
    "    for key, value in overrides.items():\n",
    "       df[key] = value\n",
    "    return df\n",
    "\n",
    "make_parameter_sheet(defaults['params']['cmd'], radar_paths.index, overrides={\n",
    "    'mission_names': mission_names,\n",
    "    'notes': transect_names\n",
    "}).to_csv(base_params_dir / 'cmd.csv')\n",
    "\n",
    "make_parameter_sheet(defaults['params']['records'], radar_paths.index, overrides={\n",
    "    'file.board_folder_name': radar_paths,\n",
    "    'gps.fn': gps_paths\n",
    "}).to_csv(base_params_dir / 'records.csv')\n",
    "\n",
    "sheets_with_defaults_only = ['qlook', 'sar', 'array', 'radar', 'post', 'analysis_noise']\n",
    "\n",
    "for sheet_name in sheets_with_defaults_only:\n",
    "    if sheet_name in defaults['params']:\n",
    "        make_parameter_sheet(defaults['params'][sheet_name], radar_paths.index).to_csv(base_params_dir / f'{sheet_name}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5865d5",
   "metadata": {},
   "source": [
    "### Generate temporary header files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712e80a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_season_subset = df_season #.loc[[('PEL', 'JKB2u', 'Y20b'), ('ICP10', 'JKB2u', 'F01T02a'), ('PEL', 'JKB2u', 'X48a')]]\n",
    "df_season_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119492e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_season_subset.iloc[0]['postprocessed_gps_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d84c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, as_completed, progress\n",
    "from dask import delayed\n",
    "\n",
    "print(\"Setting up Dask LocalCluster for parallel processing...\")\n",
    "client = Client(n_workers=10)\n",
    "print(f\"Dashboard link: {client.dashboard_link}\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bed6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_base_dir = f\"/kucresis/scratch/tteisberg_sta/scripts/opr_user_tmp/headers/rds/{season_name}/\"\n",
    "\n",
    "# Get file locations (this is fast, no need to parallelize)\n",
    "header_file_locations = df_season_subset['radar_path'].apply(opr_header_generation.get_header_file_location, base_dir=header_base_dir)\n",
    "\n",
    "# Function to get header and save it\n",
    "def get_and_save_header(path, fn):\n",
    "    \"\"\"Get header information and save it to file\"\"\"\n",
    "    header = opr_header_generation.get_header_information(path)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    fn_path = Path(fn)\n",
    "    fn_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save to file\n",
    "    hdf5storage.savemat(str(fn_path), header, format='7.3', truncate_existing=True)\n",
    "    print(f\"Saved header to {fn}\")\n",
    "    \n",
    "    return header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h0454kri986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand RADjh1 paths to include both channels (bxds1 and bxds2)\n",
    "# RADjh1 files have separate files per channel, so we need to process both\n",
    "expanded_radar_paths = []\n",
    "expanded_header_file_locations = []\n",
    "\n",
    "for path, fn in zip(df_season_subset['radar_path'], header_file_locations):\n",
    "    path_obj = Path(path)\n",
    "    \n",
    "    # Check if this is a RADjh1 file\n",
    "    if path_obj.parent.name == 'RADjh1':\n",
    "        # Add both bxds1 and bxds2\n",
    "        parent_dir = path_obj.parent\n",
    "        for channel_file in ['bxds1', 'bxds2']:\n",
    "            channel_path = parent_dir / channel_file\n",
    "            if channel_path.exists():\n",
    "                expanded_radar_paths.append(str(channel_path))\n",
    "                # Calculate header file location for this channel\n",
    "                channel_header_fn = opr_header_generation.get_header_file_location(str(channel_path), header_base_dir)\n",
    "                expanded_header_file_locations.append(channel_header_fn)\n",
    "            else:\n",
    "                print(f\"[WARNING] RADjh1 channel file not found: {channel_path}\")\n",
    "    else:\n",
    "        # Not RADjh1, keep as is\n",
    "        expanded_radar_paths.append(path)\n",
    "        expanded_header_file_locations.append(fn)\n",
    "\n",
    "print(f\"Original paths: {len(df_season_subset['radar_path'])}\")\n",
    "print(f\"Expanded paths: {len(expanded_radar_paths)} (includes both RADjh1 channels)\")\n",
    "\n",
    "# Replace the original paths with expanded paths\n",
    "radar_paths_to_process = expanded_radar_paths\n",
    "header_file_locations_to_process = expanded_header_file_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46caad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelized version using Dask\n",
    "# Create delayed tasks for each file\n",
    "delayed_tasks = []\n",
    "files_to_process = []\n",
    "\n",
    "for path, fn in zip(radar_paths_to_process, header_file_locations_to_process):\n",
    "    if Path(fn).exists():\n",
    "        print(f\"Header file already exists for {path} at {fn}, skipping.\")\n",
    "    else:\n",
    "        delayed_tasks.append(delayed(get_and_save_header)(path, fn))\n",
    "        files_to_process.append((path, fn))\n",
    "\n",
    "if len(delayed_tasks) > 0:\n",
    "    # Compute in parallel with progress bar\n",
    "    print(f\"Processing and saving {len(delayed_tasks)} header files in parallel...\")\n",
    "    \n",
    "    # Submit all tasks\n",
    "    futures = client.compute(delayed_tasks)\n",
    "    \n",
    "    # Track progress\n",
    "    progress(futures)\n",
    "    \n",
    "    # Gather results\n",
    "    headers_list = client.gather(futures)\n",
    "    \n",
    "    print(f\"Successfully generated and saved {len(headers_list)} header files.\")\n",
    "else:\n",
    "    print(\"No header files need to be generated, all files already exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d44b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
